{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39e02c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbf5718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "936e851b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_datasets(dataset):\n",
    "    # Split the dataset into training, test, and validation sets\n",
    "    train_data, test_val_data = train_test_split(dataset, test_size=0.30, random_state=42)\n",
    "    val_data,test_data = train_test_split(test_val_data, test_size=0.15, random_state=42)\n",
    "\n",
    "    return train_data, test_data, val_data\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abcd81cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "angry_train_folder = 'train/angry'\n",
    "bored_train_folder=\"train/bored\"\n",
    "angry_file = os.listdir(angry_train_folder)\n",
    "bored_file=os.listdir(bored_train_folder)\n",
    "focused_train_folder = 'train/focused'\n",
    "neutral_train_folder = 'train/neutral'\n",
    "focused_file = os.listdir(focused_train_folder)\n",
    "neutral_file = os.listdir(neutral_train_folder)\n",
    "angry_train_data, angry_test_data, angry_val_data = get_datasets(angry_file)\n",
    "bored_train_data, bored_test_data, bored_val_data = get_datasets(bored_file)\n",
    "focused_train_data, focused_test_data, focused_val_data = get_datasets(focused_file)\n",
    "neutral_train_data, neutral_test_data, neutral_val_data = get_datasets(neutral_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4628ad48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "def get_image_label_pairs(folder_path, label, transform=None):\n",
    "    image_label_pairs = []\n",
    "    files = os.listdir(folder_path)\n",
    "    for file in files:\n",
    "        if file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):  # Filter by image extensions\n",
    "            image_path = os.path.join(folder_path, file)\n",
    "            try:\n",
    "                image = cv2.imread(image_path)\n",
    "                if image is not None:\n",
    "                    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "                    # Resize the image to match the expected input size (e.g., 32x32)\n",
    "                    image = cv2.resize(image, (32, 32))\n",
    "\n",
    "                    # Convert to float and normalize\n",
    "                    image = image.astype(np.float32) / 255.0\n",
    "\n",
    "                    # Ensure the shape includes the channel dimension for PyTorch\n",
    "                    # Reshape the image to have a single channel (grayscale)\n",
    "                    image = image.reshape(1, 32, 32)\n",
    "\n",
    "                    # Convert the NumPy array to a PyTorch tensor\n",
    "                    image = torch.from_numpy(image)\n",
    "\n",
    "                    # Apply the specified transformations\n",
    "                    if transform:\n",
    "                        image = transform(image)\n",
    "\n",
    "                    image_label_pairs.append((image, label))\n",
    "                else:\n",
    "                    print(f\"Skipping {file} due to inability to read the image.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {file} due to error: {e}\")\n",
    "    return image_label_pairs\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),  # Convert NumPy array to PIL Image\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally for data augmentation\n",
    "    transforms.ToTensor(),  # Convert PIL Image to PyTorch Tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize the pixel values to the range [-1, 1]\n",
    "])\n",
    "angry_train_folder = 'train/angry'\n",
    "bored_train_folder = 'train/bored'\n",
    "focused_train_folder = 'train/focused'\n",
    "neutral_train_folder = 'train/neutral'\n",
    "\n",
    "angry_data = get_image_label_pairs(angry_train_folder, 0, transform=data_transform)\n",
    "bored_data = get_image_label_pairs(bored_train_folder, 1, transform=data_transform)\n",
    "focused_data = get_image_label_pairs(focused_train_folder, 2, transform=data_transform)\n",
    "neutral_data = get_image_label_pairs(neutral_train_folder, 3, transform=data_transform)\n",
    "\n",
    "\n",
    "angry_train, angry_test, angry_val = get_datasets(angry_data)\n",
    "bored_train, bored_test, bored_val = get_datasets(bored_data)\n",
    "focused_train, focused_test, focused_val = get_datasets(focused_data)\n",
    "neutral_train, neutral_test, neutral_val = get_datasets(neutral_data)\n",
    "\n",
    "# Combine the splits for each emotion category\n",
    "combined_training_data = angry_train  + bored_train+ neutral_data+focused_data\n",
    "combined_test_data = angry_test  + bored_test+neutral_test+focused_test\n",
    "combined_val_data = angry_val + bored_val+neutral_val+focused_val\n",
    "\n",
    "batch_size = 32  # Set your desired batch size\n",
    "custom_training_loader =DataLoader(combined_training_data, batch_size=batch_size, shuffle=True)\n",
    "custom_testing_loader=DataLoader(combined_test_data, batch_size=batch_size, shuffle=True)\n",
    "custom_validation_loader=DataLoader(combined_val_data,batch_size=batch_size,shuffle=True)\n",
    "# Reshape the data to have a single channel (assuming data is grayscale)\n",
    "print(len(bored_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88ccb43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset:\n",
      "Label 0: 568 samples\n",
      "Label 1: 454 samples\n",
      "Label 3: 800 samples\n",
      "Label 2: 758 samples\n",
      "Testing Dataset:\n",
      "Label 0: 37 samples\n",
      "Label 1: 30 samples\n",
      "Label 3: 36 samples\n",
      "Label 2: 35 samples\n",
      "Validation Dataset:\n",
      "Label 0: 207 samples\n",
      "Label 1: 165 samples\n",
      "Label 3: 204 samples\n",
      "Label 2: 193 samples\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Combined datasets (training, testing, validation)\n",
    "datasets = [combined_training_data, combined_test_data, combined_val_data]\n",
    "dataset_names = ['Training', 'Testing', 'Validation']\n",
    "\n",
    "for dataset, name in zip(datasets, dataset_names):\n",
    "    label_count = defaultdict(int)\n",
    "    for _, label in dataset:\n",
    "        label_count[label] += 1\n",
    "\n",
    "    print(f\"{name} Dataset:\")\n",
    "    for label, count in label_count.items():\n",
    "        print(f\"Label {label}: {count} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "984885ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.data[idx]\n",
    "        return img, label\n",
    "custom_training_set = CustomDataset(combined_training_data)\n",
    "custom_testing_set = CustomDataset(combined_test_data)\n",
    "custom_validation_set = CustomDataset(combined_val_data)\n",
    "\n",
    "custom_training_loader = DataLoader(custom_training_set, batch_size=batch_size, shuffle=True)\n",
    "custom_testing_loader = DataLoader(custom_testing_set, batch_size=batch_size, shuffle=True)\n",
    "custom_validation_loader = DataLoader(custom_validation_set, batch_size=batch_size, shuffle=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "786f83aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuperSimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SuperSimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=7, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=7, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Added BatchNorm2d and LeakyReLU to conv1 and conv2\n",
    "        self.batch_norm1 = nn.BatchNorm2d(16)\n",
    "        self.leaky_relu1 = nn.LeakyReLU(inplace=True)\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm2d(32)\n",
    "        self.leaky_relu2 = nn.LeakyReLU(inplace=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(32 * 5 * 5, 4)  # Adjusted input size based on the output of the last convolutional layer\n",
    "        \n",
    "                \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu1(self.batch_norm1(self.conv1(x)))\n",
    "        x = self.pool(x)  # Apply max pooling after the first convolutional layer\n",
    "\n",
    "        x = self.leaky_relu2(self.batch_norm2(self.conv2(x)))\n",
    "        x = self.pool(x)  # Apply max pooling after the second convolutional layer\n",
    "      \n",
    "        x = x.view(-1, 32 * 5 * 5)  # Adjusted input size for the fully connected layers\n",
    "        x = self.fc1(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "687fd013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed) if torch.cuda.is_available() else None\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2ba3cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "def train_model(Model,num_epochs):\n",
    "    \n",
    "    model = Model\n",
    "\n",
    "    # Inside your training loop\n",
    "    class_weights = torch.tensor([0.5, 0.5, 1.0, 0.8])\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "     # Learning rate scheduler\n",
    "\n",
    "    # Training loop\n",
    "\n",
    "    # Assuming you have combined_train_loader and validation_loader DataLoader objects\n",
    "\n",
    "    num_epochs = num_epochs\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, (images, labels) in enumerate(custom_training_loader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)  # Define criterion here with weighted classes\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (i + 1) % 100 == 0:  # Print every 100 mini-batches\n",
    "                print(f\"Epoch [{epoch + 1}, {i + 1}] Loss: {running_loss / 100:.3f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Validation after each epoch\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        val_running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in custom_validation_loader:\n",
    "                outputs = model(images)\n",
    "                val_loss = criterion(outputs, labels)\n",
    "                val_running_loss += val_loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}] Validation Loss: {val_running_loss / len(custom_validation_loader):.3f}\")\n",
    "        print(f\"Epoch [{epoch + 1}] Validation Accuracy: {(100 * correct / total):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b0fcdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1] Validation Loss: 1.264\n",
      "Epoch [1] Validation Accuracy: 33.94%\n",
      "Epoch [2] Validation Loss: 1.194\n",
      "Epoch [2] Validation Accuracy: 38.62%\n",
      "Epoch [3] Validation Loss: 1.231\n",
      "Epoch [3] Validation Accuracy: 40.70%\n",
      "Epoch [4] Validation Loss: 1.208\n",
      "Epoch [4] Validation Accuracy: 40.96%\n",
      "Epoch [5] Validation Loss: 1.157\n",
      "Epoch [5] Validation Accuracy: 42.65%\n",
      "Epoch [6] Validation Loss: 1.143\n",
      "Epoch [6] Validation Accuracy: 43.30%\n",
      "Epoch [7] Validation Loss: 1.120\n",
      "Epoch [7] Validation Accuracy: 46.55%\n",
      "Epoch [8] Validation Loss: 1.095\n",
      "Epoch [8] Validation Accuracy: 45.12%\n",
      "Epoch [9] Validation Loss: 1.079\n",
      "Epoch [9] Validation Accuracy: 46.68%\n",
      "Epoch [10] Validation Loss: 1.157\n",
      "Epoch [10] Validation Accuracy: 48.50%\n",
      "Epoch [11] Validation Loss: 1.123\n",
      "Epoch [11] Validation Accuracy: 47.85%\n",
      "Epoch [12] Validation Loss: 1.058\n",
      "Epoch [12] Validation Accuracy: 48.76%\n",
      "Epoch [13] Validation Loss: 1.098\n",
      "Epoch [13] Validation Accuracy: 50.46%\n",
      "Epoch [14] Validation Loss: 1.035\n",
      "Epoch [14] Validation Accuracy: 49.93%\n",
      "Epoch [15] Validation Loss: 1.037\n",
      "Epoch [15] Validation Accuracy: 53.06%\n",
      "Epoch [16] Validation Loss: 1.136\n",
      "Epoch [16] Validation Accuracy: 52.54%\n",
      "Epoch [17] Validation Loss: 0.970\n",
      "Epoch [17] Validation Accuracy: 52.15%\n",
      "Epoch [18] Validation Loss: 1.005\n",
      "Epoch [18] Validation Accuracy: 53.32%\n",
      "Epoch [19] Validation Loss: 0.945\n",
      "Epoch [19] Validation Accuracy: 54.88%\n",
      "Epoch [20] Validation Loss: 0.923\n",
      "Epoch [20] Validation Accuracy: 55.66%\n",
      "Epoch [21] Validation Loss: 0.954\n",
      "Epoch [21] Validation Accuracy: 57.61%\n",
      "Epoch [22] Validation Loss: 0.924\n",
      "Epoch [22] Validation Accuracy: 55.92%\n",
      "Epoch [23] Validation Loss: 0.944\n",
      "Epoch [23] Validation Accuracy: 58.13%\n",
      "Epoch [24] Validation Loss: 0.905\n",
      "Epoch [24] Validation Accuracy: 58.26%\n",
      "Epoch [25] Validation Loss: 1.000\n",
      "Epoch [25] Validation Accuracy: 56.57%\n",
      "Epoch [26] Validation Loss: 0.865\n",
      "Epoch [26] Validation Accuracy: 58.26%\n",
      "Epoch [27] Validation Loss: 1.026\n",
      "Epoch [27] Validation Accuracy: 58.91%\n",
      "Epoch [28] Validation Loss: 0.915\n",
      "Epoch [28] Validation Accuracy: 57.09%\n",
      "Epoch [29] Validation Loss: 0.828\n",
      "Epoch [29] Validation Accuracy: 62.03%\n",
      "Epoch [30] Validation Loss: 0.865\n",
      "Epoch [30] Validation Accuracy: 59.69%\n",
      "Epoch [31] Validation Loss: 0.957\n",
      "Epoch [31] Validation Accuracy: 59.17%\n",
      "Epoch [32] Validation Loss: 0.843\n",
      "Epoch [32] Validation Accuracy: 60.99%\n",
      "Epoch [33] Validation Loss: 0.812\n",
      "Epoch [33] Validation Accuracy: 63.33%\n",
      "Epoch [34] Validation Loss: 0.850\n",
      "Epoch [34] Validation Accuracy: 61.38%\n",
      "Epoch [35] Validation Loss: 0.774\n",
      "Epoch [35] Validation Accuracy: 65.54%\n",
      "Epoch [36] Validation Loss: 0.882\n",
      "Epoch [36] Validation Accuracy: 62.68%\n",
      "Epoch [37] Validation Loss: 0.810\n",
      "Epoch [37] Validation Accuracy: 65.41%\n",
      "Epoch [38] Validation Loss: 0.850\n",
      "Epoch [38] Validation Accuracy: 62.94%\n",
      "Epoch [39] Validation Loss: 0.762\n",
      "Epoch [39] Validation Accuracy: 64.37%\n",
      "Epoch [40] Validation Loss: 0.760\n",
      "Epoch [40] Validation Accuracy: 65.02%\n",
      "Epoch [41] Validation Loss: 0.824\n",
      "Epoch [41] Validation Accuracy: 62.16%\n",
      "Epoch [42] Validation Loss: 0.774\n",
      "Epoch [42] Validation Accuracy: 63.59%\n",
      "Epoch [43] Validation Loss: 0.799\n",
      "Epoch [43] Validation Accuracy: 64.11%\n",
      "Epoch [44] Validation Loss: 0.718\n",
      "Epoch [44] Validation Accuracy: 66.97%\n",
      "Epoch [45] Validation Loss: 0.732\n",
      "Epoch [45] Validation Accuracy: 66.71%\n"
     ]
    }
   ],
   "source": [
    "SuperSimpleCNN_model=SuperSimpleCNN()\n",
    "train_model(Model=SuperSimpleCNN_model,num_epochs=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6a18259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "def load_model_and_predict(model, image_path):\n",
    "    # Add more elif statements for other models as needed\n",
    "    model.eval()\n",
    "\n",
    "    transformation = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally for data augmentation\n",
    "        transforms.Resize((32, 32)),  # Resize the image to (32, 32)\n",
    "        transforms.ToTensor(),  # Convert PIL Image to PyTorch Tensor\n",
    "        transforms.Normalize((0.5,), (0.5,))  # Normalize the pixel values to the range [-1, 1]\n",
    "    ])\n",
    "\n",
    "    image = Image.open(image_path).convert(\"L\")  # Convert to grayscale\n",
    "    image = transformation(image)\n",
    "    with torch.no_grad():\n",
    "        predictions = model(image.unsqueeze(0))  # Add an extra dimension for batch size\n",
    "\n",
    "        \n",
    "    # Print or use predictions as needed\n",
    "    probabilities = F.softmax(predictions, dim=1)\n",
    "    confidence, predicted_class = torch.max(probabilities, 1)\n",
    "    predicted_class = predicted_class.item()\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Predicted class: {predicted_class}, Confidence: {confidence.item()}\")\n",
    "\n",
    "    return predicted_class, confidence.item()\n",
    "    print(predictions.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b0209cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] image_path\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"Load and run a saved PyTorch model on an image.\")\n",
    "    parser.add_argument(\"image_path\", type=str, help=\"Path to the input image\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    load_model_and_predict(SuperSimpleCNN_model, args.image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1b6ce4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
